{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./venv/lib/python3.10/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./venv/lib/python3.10/site-packages (from faiss-cpu) (2.2.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.1.0 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in ./venv/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.10/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.10/site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.10/site-packages (from groq) (2.10.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./venv/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in ./venv/lib/python3.10/site-packages (13.9.4)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in ./venv/lib/python3.10/site-packages (from rich) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "!pip install groq\n",
    "!pip install rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the dataset to be stored to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = {\n",
    "    \"Name\": \"TechShield\",\n",
    "    \"Misson\": \"To empower Small and Medium Enterprises (SMEs) with robust cybersecurity solutions.\",\n",
    "    \"Focus\": \"Addresses common SME challenges like limited resources, complex compliance, and inefficient incident response.\",\n",
    "    \"Key Features\": \"\"\"\n",
    "        AI-Powered Threat Detection & Prevention:\n",
    "            - Real-time monitoring for anomalies and potential threats.\n",
    "            - Automated patch management for vulnerabilities.\n",
    "        Compliance as a Service (CaaS):\n",
    "            - Simplified tracking of major regulations (e.g., GDPR, HIPAA, PCI-DSS).\n",
    "            - Automated audit readiness reports.\n",
    "        Incident Response & Management (IR&M):\n",
    "            - Pre-configured response plans for common incidents.\n",
    "            - 24/7 access to cybersecurity experts.\n",
    "        User Awareness & Training:\n",
    "            - Phishing simulations and training exercises.\n",
    "            - Cybersecurity knowledge base and workshops.\n",
    "    \"\"\",\n",
    "    \"Business Model\":\n",
    "        {\n",
    "            \"Revenue Streams\": \"\"\"\n",
    "                - Subscription model with tiered pricing (Basic, Premium, Enterprise).\n",
    "                - Enhanced support packages.\n",
    "                - Custom compliance solutions.\n",
    "                - Incident response retainer fees.\n",
    "            \"\"\",\n",
    "            \"Marketing Strategies\": \"\"\"\n",
    "                - Content marketing (white papers, webinars, case studies).\n",
    "                - Partnerships with Managed Service Providers (MSPs) and industry associations.\n",
    "                Targeted online advertising.\n",
    "            \"\"\"\n",
    "        },\n",
    "    \"Development Roadmap\": \"\"\"\n",
    "    - Phase 1 (Months 1-3): Team building, market research, platform design.\n",
    "    - Phase 2 (Months 4-9): Development of core features (threat detection, CaaS, IR&M, training).\n",
    "    - Phase 3 (Months 10-12): Testing, iteration, and launch preparation.\n",
    "    - Launch & Growth (Quarter 2 onwards): Go-to-market, marketing campaigns, continuous improvement.\n",
    "    \"\"\",   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepairing and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep=' '):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "flattened_dataset = flatten_dict(DATASET)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=\"./cache\")\n",
    "\n",
    "# Create embeddings for each item in the dataset\n",
    "texts = list(flattened_dataset.values())\n",
    "embeddings = model.encode(texts).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Store Data in Faiss\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Store the text and their corresponding embeddings\n",
    "text_to_embedding = {text: embedding for text, embedding in zip(texts, embeddings)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save the embeddings and index\n",
    "def save_embeddings_and_index(embeddings, index, text_to_embedding, embeddings_file='embeddings.pkl', index_file='faiss_index.bin'):\n",
    "    # Save embeddings\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        pickle.dump((texts, embeddings, text_to_embedding), f)\n",
    "    \n",
    "    # Save index\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "save_embeddings_and_index(embeddings, index, text_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load the embeddings and index\n",
    "def load_embeddings_and_index(embeddings_file='embeddings.pkl', index_file='faiss_index.bin'):\n",
    "    # Load embeddings\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        texts, embeddings, text_to_embedding = pickle.load(f)\n",
    "    \n",
    "    # Load index\n",
    "    index = faiss.read_index(index_file)\n",
    "    \n",
    "    return texts, embeddings, text_to_embedding, index\n",
    "\n",
    "texts, embeddings, text_to_embedding, index = load_embeddings_and_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Retrieve Data\n",
    "def retrieve_data(query, index, text_to_embedding, top_k=1):\n",
    "    # Create embedding for the query\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    \n",
    "    # Retrieve the most similar items\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the corresponding texts\n",
    "    retrieved_texts = [list(text_to_embedding.keys())[i] for i in indices[0]]\n",
    "    \n",
    "    return retrieved_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Process Retrieved Data with GROQ\n",
    "GROQ_API_KEY = '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'\n",
    "\n",
    "# Initialize the GROQ client\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def generate_response(query, retrieved_texts):\n",
    "    # Combine the query and retrieved texts\n",
    "    system_prompt = f\"here is the context provided to you for the query.\\\n",
    "        the context provided to you Is your own knowledge. you are working\\\n",
    "        as a reseptionist at a company and are providing answer to a person who is aking you\\\n",
    "        so keep your language in that manner. context:{retrieved_texts}\"\n",
    "    context = f\"Query: {query}\"\n",
    "    \n",
    "    # Generate response using GROQ (replace with actual GROQ API call)\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": context\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "    \n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how does your business plan to earn?\"\n",
    "retrieved_texts = retrieve_data(query, index, text_to_embedding)\n",
    "response = generate_response(query, retrieved_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "def md(text):\n",
    "    console = Console()\n",
    "    return console.print(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how does your business plan to earn?\n",
      "Retrieved Texts: ['\\n                - Content marketing (white papers, webinars, case studies).\\n                - Partnerships with Managed Service Providers (MSPs) and industry associations.\\n                Targeted online advertising.\\n            ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Texts:\", retrieved_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">At our company, we have a robust revenue strategy in place to ensure a steady influx of income. Firstly, we focus  \n",
       "on content marketing efforts, specifically creating high-quality white papers, webinars, and case studies that     \n",
       "provide immense value to our target audience. These valuable resources attract potential customers, and through    \n",
       "targeted advertising, we direct them to our website and subsequently generate leads.                               \n",
       "\n",
       "Additionally, we've established partnerships with Managed Service Providers (MSPs) and industry associations,      \n",
       "creating a win-win situation for both parties. We offer expertise, and they bring in new clients. This synergy     \n",
       "helps us expand our reach and improve our offerings.                                                               \n",
       "\n",
       "Lastly, we utilize targeted online advertising to reach our ideal customer base, focusing on platforms that yield  \n",
       "the highest conversions. By having a multi-channel approach, we're confident that our revenue streams will be      \n",
       "diverse and consistent, allowing us to scale our operations and continue investing in growth initiatives.          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "At our company, we have a robust revenue strategy in place to ensure a steady influx of income. Firstly, we focus  \n",
       "on content marketing efforts, specifically creating high-quality white papers, webinars, and case studies that     \n",
       "provide immense value to our target audience. These valuable resources attract potential customers, and through    \n",
       "targeted advertising, we direct them to our website and subsequently generate leads.                               \n",
       "\n",
       "Additionally, we've established partnerships with Managed Service Providers (MSPs) and industry associations,      \n",
       "creating a win-win situation for both parties. We offer expertise, and they bring in new clients. This synergy     \n",
       "helps us expand our reach and improve our offerings.                                                               \n",
       "\n",
       "Lastly, we utilize targeted online advertising to reach our ideal customer base, focusing on platforms that yield  \n",
       "the highest conversions. By having a multi-channel approach, we're confident that our revenue streams will be      \n",
       "diverse and consistent, allowing us to scale our operations and continue investing in growth initiatives.          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
